{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#参考情報\" data-toc-modified-id=\"参考情報-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>参考情報</a></span></li><li><span><a href=\"#迷路\" data-toc-modified-id=\"迷路-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>迷路</a></span></li><li><span><a href=\"#Q学習\" data-toc-modified-id=\"Q学習-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Q学習</a></span></li><li><span><a href=\"#インポート\" data-toc-modified-id=\"インポート-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>インポート</a></span></li><li><span><a href=\"#クラス定義\" data-toc-modified-id=\"クラス定義-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>クラス定義</a></span><ul class=\"toc-item\"><li><span><a href=\"#環境\" data-toc-modified-id=\"環境-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>環境</a></span></li><li><span><a href=\"#エージェント\" data-toc-modified-id=\"エージェント-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>エージェント</a></span><ul class=\"toc-item\"><li><span><a href=\"#標準\" data-toc-modified-id=\"標準-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>標準</a></span></li><li><span><a href=\"#UCB1-(Upper-Confidence-Bound-(信頼上限)-のバージョン-1)\" data-toc-modified-id=\"UCB1-(Upper-Confidence-Bound-(信頼上限)-のバージョン-1)-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>UCB1 (Upper Confidence Bound (信頼上限) のバージョン 1)</a></span></li><li><span><a href=\"#DQN\" data-toc-modified-id=\"DQN-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>DQN</a></span></li></ul></li></ul></li><li><span><a href=\"#プロット\" data-toc-modified-id=\"プロット-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>プロット</a></span></li><li><span><a href=\"#シミュレーション\" data-toc-modified-id=\"シミュレーション-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>シミュレーション</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考情報\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迷路\n",
    "<img src=\"Meiro.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q学習\n",
    "<img src=\"Q_Learning.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "オレンジのパスがQ値を最大にするパスである場合、$Q(s_0,b)$の割引現在価値は、\n",
    "$$\\begin{eqnarray}\n",
    "Q(s_0,b) &=& r_2 + \\gamma Q(s_2,a)  \\\\\n",
    "         &=& r_2 + \\gamma r_5 + \\gamma^2 Q(s_5,a) \\\\\n",
    "         &=& r_2 + \\gamma r_5 + \\gamma^2 r_9\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アルファ碁解体新書(p.140)によれば\n",
    "<img src=\"QLearning_Fig.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pickle\n",
    "from IPython import display\n",
    "#from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラス定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enviro:\n",
    "    def __init__(self):\n",
    "        self.state = {}\n",
    "        self.state[0] = [ 0, 1, 4, 0]\n",
    "        self.state[1] = [ 1, 2, 1, 0]\n",
    "        self.state[2] = [ 2, 3, 2, 1]\n",
    "        self.state[3] = [ 3, 3, 7, 2]\n",
    "        self.state[4] = [ 0, 5, 8, 4]\n",
    "        self.state[5] = [ 5, 6, 9, 4]\n",
    "        self.state[6] = [ 6, 6, 10, 5]\n",
    "        self.state[7] = [ 3, 7, 11, 7]\n",
    "        self.state[8] = [ 4, 8, 8, 8]\n",
    "        self.state[9] = [ 5, 9, 13, 9]\n",
    "        self.state[10] = [ 6, 10, 14, 10]\n",
    "        self.state[11] = [ 7, 11, 11, 11]\n",
    "        self.state[12] = [ 12, 13, 12, 12]\n",
    "        self.state[13] = [ 9, 13, 13, 12]\n",
    "        self.state[14] = [ 10, 15, 14, 14]\n",
    "        self.state[15] = [ 15, 16, 15, 14]\n",
    "        self.goal = 16\n",
    "        self.prev_state = -1\n",
    "        \n",
    "    def move(self, current_state, action ):\n",
    "        next_state = self.state[current_state][action]\n",
    "        if next_state == self.goal:\n",
    "            reward = 1.0\n",
    "        elif ( current_state == next_state ):\n",
    "            reward = -1.0\n",
    "        else:\n",
    "            reward = 0\n",
    "        self.prev_state = current_state\n",
    "        return next_state, reward\n",
    "\n",
    "    def get_goal(self):\n",
    "        return self.goal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エージェント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Agent:\\n    def __init__(self):\\n        self.qt = np.zeros( [17, 4] )\\n        self.visit = np.ones( [17, 4] )\\n        self.eps = 0.4\\n        self.gamma = 0.9\\n        self.alp = 0.01\\n\\n    def action(self, state):\\n        ngame = np.sum( self.visit[15][1] )\\n        if np.random.rand() < self.eps * 1.386 / np.log(ngame):\\n            act = int( np.random.rand() * 4 )\\n        else:\\n            act = np.argmax( self.qt[state] )\\n        return act\\n\\n    def action_ucb1(self, state):\\n        total_visit = np.sum( self.visit[state] )\\n        total_rewards = np.sum( self.qt[state] ) + 1e-10\\n#        ucb1 = self.qt[state] / total_rewards + #               np.sqrt( 2.0 * np.log(total_visit / self.visit[state] ) )\\n\\n        ucb1 = self.qt[state] +                np.sqrt( ( 2.0 * np.log(total_visit )  ) / self.visit[state] )\\n\\n#        print( state, total_visit, total_rewards, ucb1 )\\n        act = np.argmax( ucb1 )\\n        return act\\n\\n    def update( self, src, act, dst, reward ):\\n        maxq = np.max( self.qt[dst])\\n        td_diff = reward + self.gamma * maxq - self.qt[src,act]\\n        self.qt[src,act] += self.alp * td_diff\\n        self.visit[src,act] += 1\\n\\n    def batch_update( self, states ):\\n        pass\\n    \\n    def get_qt(self):\\n        return self.qt\\n\\n    def get_visit(self):\\n        return self.visit\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.qt = np.zeros( [17, 4] )\n",
    "        self.visit = np.ones( [17, 4] )\n",
    "        self.eps = 0.4\n",
    "        self.gamma = 0.9\n",
    "        self.alp = 0.01\n",
    "\n",
    "    def action(self, state):\n",
    "        ngame = np.sum( self.visit[15][1] )\n",
    "        if np.random.rand() < self.eps * 1.386 / np.log(ngame):\n",
    "            act = int( np.random.rand() * 4 )\n",
    "        else:\n",
    "            act = np.argmax( self.qt[state] )\n",
    "        return act\n",
    "\n",
    "    def action_ucb1(self, state):\n",
    "        total_visit = np.sum( self.visit[state] )\n",
    "        total_rewards = np.sum( self.qt[state] ) + 1e-10\n",
    "#        ucb1 = self.qt[state] / total_rewards + \\\n",
    "#               np.sqrt( 2.0 * np.log(total_visit / self.visit[state] ) )\n",
    "\n",
    "        ucb1 = self.qt[state] + \\\n",
    "               np.sqrt( ( 2.0 * np.log(total_visit )  ) / self.visit[state] )\n",
    "\n",
    "#        print( state, total_visit, total_rewards, ucb1 )\n",
    "        act = np.argmax( ucb1 )\n",
    "        return act\n",
    "\n",
    "    def update( self, src, act, dst, reward ):\n",
    "        maxq = np.max( self.qt[dst])\n",
    "        td_diff = reward + self.gamma * maxq - self.qt[src,act]\n",
    "        self.qt[src,act] += self.alp * td_diff\n",
    "        self.visit[src,act] += 1\n",
    "\n",
    "    def batch_update( self, states ):\n",
    "        pass\n",
    "    \n",
    "    def get_qt(self):\n",
    "        return self.qt\n",
    "\n",
    "    def get_visit(self):\n",
    "        return self.visit\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標準"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, goal ):\n",
    "        self.qt = np.zeros( [goal+1, 4] )\n",
    "        self.visit = np.ones( [goal+1, 4] ) # UCB1のゼロ割回避のため\n",
    "        self.eps = 0.4\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.01\n",
    "\n",
    "    def action(self, state):\n",
    "        #ngame = np.sum( self.visit[15][1] )\n",
    "        if np.random.rand() < self.eps: # * 1.386 / np.log(ngame):\n",
    "            act = int( np.random.rand() * 4 )\n",
    "        else:\n",
    "            act = np.argmax( self.qt[state] )\n",
    "        return act\n",
    "\n",
    "    def update( self, src, act, dst, reward ):\n",
    "        maxq = np.max( self.qt[dst])\n",
    "        td_diff = reward + self.gamma * maxq - self.qt[src,act]\n",
    "        self.qt[src,act] += self.alpha * td_diff\n",
    "        self.visit[src,act] += 1\n",
    "\n",
    "    def batch_update( self, states, goal ): # for DQN\n",
    "        pass\n",
    "    \n",
    "    def get_qt(self):\n",
    "        return self.qt\n",
    "\n",
    "    def get_visit(self):\n",
    "        return self.visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB1 (Upper Confidence Bound (信頼上限) のバージョン 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class AgentUcb1(Agent):\n",
    "    def action(self, state):\n",
    "        total_visit = np.sum( self.visit[state] )\n",
    "        total_rewards = np.sum( self.qt[state] ) + 1e-10\n",
    "#        ucb1 = self.qt[state] / total_rewards + \\\n",
    "#               np.sqrt( 2.0 * np.log(total_visit / self.visit[state] ) )\n",
    "\n",
    "        ucb1 = self.qt[state] + \\\n",
    "               np.sqrt( ( 2.0 * np.log(total_visit )  ) / self.visit[state] )\n",
    "\n",
    "#        print( state, total_visit, total_rewards, ucb1 )\n",
    "        act = np.argmax( ucb1 )\n",
    "        return act\n",
    "\"\"\"\n",
    "class AgentUcb1(Agent):\n",
    "    def action(self, state):\n",
    "        ucb1 = get_ucb1( self.qt[state], self.visit[state] )\n",
    "        act = np.argmax( ucb1 )\n",
    "        return act\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ucb1( qts, visits ):\n",
    "    total_visit = np.sum( visits )\n",
    "    ucb1 = qts + np.sqrt( ( 2.0*np.log(total_visit) ) / visits )\n",
    "    return ucb1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, clone_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class AgentDQN(Agent):\n",
    "    def __init__(self, goal, nbatch=32 ):\n",
    "        super().__init__(goal)\n",
    "        self.nbatch = nbatch\n",
    "        self.goal = goal\n",
    "        self.fixed_cnt = 0\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, activation='relu', input_shape=(goal+1,)))  \n",
    "        self.model.add(Dense(64, activation='relu' ) )\n",
    "        self.model.add(Dense(4) ) \n",
    "        #self.model.add(Dense(4, activation='relu') ) # 負の価値を避ける\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=\"adam\", loss='mean_squared_error' )\n",
    "\n",
    "        self.fixed_model = clone_model(self.model)\n",
    "        self.fixed_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def action(self, state):\n",
    "        x_test, y_pred = self._ohe_forward( [state], self.model )\n",
    "        #print( \"#action \", state, x_test, y_pred )\n",
    "        ucb1 = get_ucb1( y_pred, self.visit[state])\n",
    "        return np.argmax( ucb1 )\n",
    "    \n",
    "    def update( self, src, act, dst, reward ):\n",
    "        self.visit[src,act] += 1\n",
    "        \n",
    "    def batch_update( self, exp, goal ): # for DQN\n",
    "        #expidx = np.random.choice( len(exp['state']), self.nbatch ) # Experience replay\n",
    "        expidx = np.arange( len(exp['state']) ) # Use All\n",
    "\n",
    "        states = np.array( exp['state'], dtype='int')[expidx]\n",
    "        acts = np.array( exp['act'], dtype='int')[expidx]\n",
    "        rewards = np.array( exp['reward'], dtype='float')[expidx]\n",
    "        new_states = np.array( exp['new_state'], dtype='int')[expidx]\n",
    "        \n",
    "        print( \"#states\\n\", states )\n",
    "        print( \"#acts\\n\", acts )\n",
    "        print( \"#rewards\\n\", rewards )\n",
    "        print( \"#new_states\\n\", new_states )\n",
    "\n",
    "        x_train, y_train = self._ohe_forward( states, self.model, train=True )\n",
    "        _, y_pred_new = self._ohe_forward( new_states, self.fixed_model, train=True )\n",
    "        \n",
    "        y_pred_new_max = np.max( y_pred_new, axis=1 )\n",
    "        \n",
    "        #print( '#state/act/y_pred [last]', states[-1], acts[-1], y_pred[-1][acts[-1]] )\n",
    "        ##print( '#new_states[last], y_pred_new_max[last]\\n', new_states[-1], y_pred_new_max[-1] )    \n",
    "        #print( '#y_pred Actual\\n', y_pred )\n",
    "        #print( '#y_pred_next Actual(Fixed)\\n', y_pred_new )\n",
    "        for i,yi in enumerate(y_train):\n",
    "            #print( i, y, acts[i], rewards[i], y_pred_new_max[i] )\n",
    "            yi[acts[i]] = rewards[i] + self.gamma * y_pred_new_max[i] # 選択した行動の現在価値を見積もりの現在価値で上書き。選択しなかったものはそのまま。\n",
    "        print( '#x_train\\n', x_train)    \n",
    "        print( '#y_pred Ideal\\n', y_train )\n",
    "\n",
    "        loss = self.model.train_on_batch(x_train, y_train)\n",
    "        self.fixed_cnt += 1\n",
    "        \n",
    "        # Fixed Target Q-Network の更新\n",
    "        if self.fixed_cnt > 3:\n",
    "            self.fixed_cnt = 0\n",
    "            print( '*** Update Fixed Target Q-Network ***')\n",
    "            self.fixed_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def _ohe_forward( self, states, mdl, train=False ):\n",
    "        \"\"\" Forward with One Hot Encoding \"\"\"\n",
    "        x_test = np.zeros( (len(states), self.goal+1 ) ) # Goalを含む\n",
    "        for idx,st in enumerate(states):\n",
    "            x_test[idx][st] = 1\n",
    "        y_pred = mdl.predict(x_test)\n",
    "        if train == True:\n",
    "            pos = np.array(states) == self.goal\n",
    "            y_pred[pos] = 0\n",
    "            #print( \"len(states) #pos= \", len(states), np.where(pos) )\n",
    "        return x_test, y_pred\n",
    "\n",
    "    def get_qt(self):\n",
    "        _,y_pred = self._ohe_forward( np.arange(self.goal), self.model)\n",
    "        self.qt[:-1][:] = y_pred\n",
    "        return self.qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# プロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressPlot:\n",
    "    def __init__(self):\n",
    "        self.fig, self.ax = plt.subplots( 1,3, figsize=[8,5])\n",
    "        plt.show()\n",
    "    \n",
    "    def update( self, qt_tbl, visit_tbl, cnt_list ):\n",
    "        self.ax[0].clear()\n",
    "        self.ax[0].imshow( qt_tbl, cmap='copper' )\n",
    "        #self.ax[0].invert_yaxis()\n",
    "        self.ax[0].xaxis.set_major_locator(ticker.MultipleLocator(1.0))\n",
    "        self.ax[0].yaxis.set_major_locator(ticker.MultipleLocator(1.0))\n",
    "        self.ax[0].set_title( \"Q Table\" )\n",
    "\n",
    "        self.ax[1].clear()\n",
    "        self.ax[1].imshow( visit_tbl, cmap='copper' )\n",
    "        #self.ax[1].invert_yaxis()\n",
    "        self.ax[1].xaxis.set_major_locator(ticker.MultipleLocator(1.0))\n",
    "        self.ax[1].yaxis.set_major_locator(ticker.MultipleLocator(1.0))\n",
    "        self.ax[1].set_title( \"number of visit\" )\n",
    "\n",
    "        self.ax[2].clear()\n",
    "        self.ax[2].loglog( cnt_list )\n",
    "        self.ax[2].grid( which='both', axis='both')\n",
    "        self.ax[2].set_title( \"%d episods %d steps\" % (len(cnt_list),cnt_list[-1]))\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(self.fig)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# シミュレーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a8ae6417a5aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgoal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_goal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgentDQN\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mgoal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgoal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#ag = AgentUcb1( goal=goal )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprogress_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProgressPlot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-1fbb7cd9587f>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, goal, nbatch)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgoal\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                     \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 463\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    893\u001b[0m                                       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'kernel'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                                       \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m                                       constraint=self.kernel_constraint)\n\u001b[0m\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m             self.bias = self.add_weight(shape=(self.units,),\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[0;32m    280\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m                             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m                             constraint=constraint)\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weight_regularizer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mvariable\u001b[1;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \"\"\"\n\u001b[0;32m    619\u001b[0m     v = tf_keras_backend.variable(\n\u001b[1;32m--> 620\u001b[1;33m         value, dtype=dtype, name=name, constraint=constraint)\n\u001b[0m\u001b[0;32m    621\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tocoo'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mvariable\u001b[1;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m    815\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[1;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         shape=shape)\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(**kws)\u001b[0m\n\u001b[0;32m    233\u001b[0m                         shape=None):\n\u001b[0;32m    234\u001b[0m     \u001b[1;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m   2643\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2644\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2645\u001b[1;33m       shape=shape)\n\u001b[0m\u001b[0;32m   2646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m   1409\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1410\u001b[0m           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1411\u001b[1;33m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[0;32m   1412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m   def _init_from_args(self,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[0;32m   1555\u001b[0m               \u001b[0mshared_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1556\u001b[0m               \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1557\u001b[1;33m               graph_mode=self._in_graph_mode)\n\u001b[0m\u001b[0;32m   1558\u001b[0m         \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m         if (self._in_graph_mode and initial_value is not None and\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36meager_safe_variable_handle\u001b[1;34m(initial_value, shape, shared_name, name, graph_mode)\u001b[0m\n\u001b[0;32m    230\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m   return _variable_handle_from_shape_and_dtype(\n\u001b[1;32m--> 232\u001b[1;33m       shape, dtype, shared_name, name, graph_mode, initial_value)\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_variable_handle_from_shape_and_dtype\u001b[1;34m(shape, dtype, shared_name, name, graph_mode, initial_value)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;31m# support string tensors, we encode the assertion string in the Op name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     gen_logging_ops._assert(  # pylint: disable=protected-access\n\u001b[1;32m--> 164\u001b[1;33m         math_ops.logical_not(exists), [exists], name=\"EagerVariableNameReuse\")\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0mhandle_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcpp_shape_inference_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCppShapeInferenceResult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHandleData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_logging_ops.py\u001b[0m in \u001b[0;36m_assert\u001b[1;34m(condition, data, summarize, name)\u001b[0m\n\u001b[0;32m     44\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m     45\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Assert\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         tld.op_callbacks, condition, data, \"summarize\", summarize)\n\u001b[0m\u001b[0;32m     47\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Experience = namedtuple( \"Experience\", \"state act reward new_state\" )\n",
    "\n",
    "def reset_exp():\n",
    "    return { \"state\":[], \"act\":[], \"reward\":[], \"new_state\":[] }\n",
    "\n",
    "ev = Enviro()\n",
    "goal = ev.get_goal()\n",
    "\n",
    "ag = AgentDQN( goal=goal, nbatch=50 )\n",
    "#ag = AgentUcb1( goal=goal )\n",
    "progress_plot = ProgressPlot()\n",
    "\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots( 1,3, figsize=[8,5])\n",
    "plt.show()\n",
    "\n",
    "nepi = 4000\n",
    "batch_step = 100 \n",
    "max_step = 1000\n",
    "cntacc = 0\n",
    "cnts = []\n",
    "exp = reset_exp()\n",
    "\n",
    "for epi in range(nepi):\n",
    "    state = 0\n",
    "    cnt = 0\n",
    "    while (state < goal) and ( cnt < max_step ):\n",
    "        act = ag.action(state)\n",
    "        new_state, reward = ev.move(state,act)\n",
    "        ag.update( state, act, new_state, reward )\n",
    "        # For batch_update\n",
    "        exp[\"state\"].append( state )\n",
    "        exp[\"act\"].append( act )\n",
    "        exp[\"reward\"].append( reward )\n",
    "        exp[\"new_state\"].append( new_state )\n",
    "        # For next Loop\n",
    "        state = new_state\n",
    "        cnt += 1\n",
    "    cnts.append( cnt )\n",
    "    cntacc += cnt\n",
    "    if cntacc >= batch_step:\n",
    "        cntacc = 0\n",
    "        ag.batch_update( exp, goal )\n",
    "        exp = reset_exp()\n",
    "        #progress_plot.update( ag.get_qt()[:-1,:], ag.get_visit()[:-1,:], cnts) # 最終行はガードなので非表示\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ag.get_qt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_plot = ProgressPlot()\n",
    "pr_plot.update( ag.get_qt()[:-1,:], ag.get_visit()[:-1,:], cnts) # 最終行はガードなので非表示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
